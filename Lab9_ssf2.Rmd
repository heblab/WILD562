---
title: "Lab 9 - Advanced SSF Models"
author: "Mark Hebblewhite"
date: "12 March 2019"
output: 
  html_document:
    theme: simplex
    toc: true
    toc_float: true
    number_sections: true
    self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
knitr::opts_knit$set(root.dir = '/Users/mark.hebblewhite/Box Sync/Teaching/UofMcourses/WILD562/Spring2019/Labs/Lab9/2019/')
```

## Preliminaries: setting packages
```{r load packages}
library(sp)
library(raster)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(mapview)
library(maptools)
library(leaflet)
library(magrittr)
#install.packages("amt")
## note have to do this in the Lab
#options(buildtools.check = function(action) TRUE)
#library(devtools)
#devtools::install_github("jmsigner/amt")
library(amt)
```

Note that the working directory for Lab 9 is set locally on your computer, but that I have set it here globally, so output is supressed. Also note that it may be necessary to install amt directly from github, depending on updates and conflicts with dplyr. 

# Lab 9 - Advanced Step Selection Function Models

In todays lab we will continue to build skills modeling resource selection using movement-based statistical models. In particular, we will continue with the amt package for a case study of Fishers, and fit integrated step selection functions (iSSF) to individual and multiple individuals from a well known Fisher dataset.  Today's lab objectives are:

1) Explore the foundation for iSSF models, Biased-Correlated Random Walk models.
2) Fit an iSSF to a single Fisher dataset, estimating a clogit model, as well as then fitting the resultant movement and habitat kernels to field data.
3) Fit an iSSF to multiple individual Fishers. 
4) Explore mixed-effects cLogit models using the coxme and mcclogit packages.
 

# Biased-Correlated Random Walks

_From Fagan and Calabrese - Bulletin of the Ecological Society of America 2014_

More than 30 years ago, an early, sturdy bridge between field data and spatial ecological theory was built when the article “Analyzing insect movement as a correlated random walk” was published in Oecologia. This paper, which represented a collaboration between ecologist Peter Kareiva and mathematician Nanako Shigesada, is a milestone along the Paper Trail because it marks a critical link between the abstract world of ecological theory and the hands-on way in which ecologists actually collect data on individual animals.

This correlated random walk model was comprised of steps and turns, and Kareiva and Shigesada showed how one could estimte these distributions, and, make them functions of spatial or temporal covariates through field data.  The biased correlated random walk emerged, and represents the cornerstone of the step dplyr::selection function concept. And it links the mechanistic movement models of Moorcroft, Lewis and Barnett to field data approaches commonly collected with GPS data. 

In this first excercise, we will explore how different 'parameters' of movement (step, turns) and bias towards a centroid influence the spatial pattern of movement.  In essence, the BCRW is the driver of the movement kernel distribution in the iSSF models we will use from the package amt. 

First, we make a function that draws random movements based on 3 parameters, a, b, rho (the degree of correaltion), and an attraction to a home range activity center.  a and b are parameters of the step lenght distribution, fit as a Weibull distribution. Larger values of a or b represent more or less longer step lenghts.  Rho is the degree of directional persistence or 'bias' in the correlation in direction between steps, and the attraction is equivalent to the mathematical advection term in Moorcroft and Barnett.  

Here, we will compare just 3 types of fits to explore, but I encourage you to play around with the paramters on your own to get a feel for unbiased and biased correlated random walks. 
```{r cars}
#### Correlated Random Walks
BCRW <- function(a = 2, b = 1, rho = 0.7, Z.center = 0, attraction = 0.5, n = 50, Z0 = 0){
  require(CircStats)
  
  Z <- c(Z0, rep(NA,n-1))
  phi <- runif(1, -pi, pi)
  for(i in 2:n)
  {
    # relative orientation to center of attraction
    chi <- Arg(Z.center - Z[i-1])
    dphi <- chi-phi
    if(abs(chi - phi) > pi) dphi <- (chi - phi) - pi
    
    # adjust the location 
    location <- phi + attraction * dphi
    
    # pick a new absolute direction ... but MUST BE BETWEEN -pi and pi
    phi <- rwrpcauchy(1, location, rho) - 2*pi
    if(phi > pi) phi <- phi-2*pi
    if(phi < -pi) phi <- phi+2*pi
    
    Z[i] <- Z[i-1] + complex(arg = phi, mod = rweibull(1, a, b))
  }
  return(Z)
}

BCRW(a = 2, b = 1, rho = 0, Z.center = 10, attraction = 0.25, n = 2000) %>% 
  plot(type="o", asp=1, pch = 21, bg= grey(seq(0,1,length = 2000)),
       main = "a = 2, b = 1, rho = 0.2, attraction = 0")

BCRW(a = 2, b = 1, rho = 0.5, Z.center = 10, attraction = 0.25, n = 2000) %>% 
  plot(type="o", asp=1, pch = 21, bg= grey(seq(0,1,length = 200)),
       main = "a = 2, b = 1, rho = 0.7, attraction = 0.5")

BCRW(a = 2, b = 1, rho = 0.7, Z.center = 10, attraction = 0.25, n = 2000) %>% 
  plot(type="o", asp=1, pch = 21, bg= grey(seq(0,1,length = 200 )),
       main = "a = 2, b = 1, rho = 0.7, attraction = 0.8")

```

These three little simulations demonstrate the effects of varying correlation, or bias, in the step lengths for the same step length distribution (here, fit by an a and b), and the same measure of home range attraction.  It is this biased CRW that forms the basis of the simulation in the iSSF that generates the spatial predictions of the expected utilization distribution, based on the 'bias' terms made explicity from the habitat selection and movement process. 

# Animal movement tools (amt)
Next, we will start using the amt:: R package for managing tracking data and conducting habitat selection analyses.

We will estimate a Step Selection Function, and, an integrated SSF to map the predictions of spatial distribution from the SSF based on the work of Tal Avgar et al's integrated Step Selection Function modeling approach.  This will be based on the examples provided in this paper by:
  Singer, J., Fieberg, J. & Avgar, T. (2019) Animal movement tools (amt): R package for       managing tracking data and conducting habitat selection analyses. Ecol Evol, 9, 880-890.

based on this method:
  Avgar, T., Potts, J.R., Lewis, M.A., Boyce, M.S. & Börger, L. (2016) Integrated step        dplyr::selection analysis: bridging the gap between resource selection and animal movement. Methods in Ecology and Evolution, 7, 619-630.

These data are from Fishers in upstate New York collected by a colleague, Scott La Point, during his PhD with Max Planck.  To get started, we will review the handy Figure from Avgar et al. about the steps involved in fitting an integrated Step Seletion Function from animal movement data.

![Figure 9.1. iSSF workflow](/Users/mark.hebblewhite/Box Sync/Teaching/UofMcourses/WILD562/Spring2019/Labs/lab9/2019/AvgarFig1.jpg)

Lets get started....
```{r}
fisher <- read.csv("Martes pennanti LaPoint New York.csv")
#head(fisher)
#str(fisher)
ggplot(fisher, aes(location.long, location.lat, colour = individual.local.identifier)) + geom_point()
```

Next, let's turn fisher into a spatial points data frame for use in raster operations later. First we need to remove NA's, which we can do here using complete.cases and which we do later using the filter(!is.na()) command.
```{r}
fisher1<-fisher[complete.cases(fisher[4:5]),]
xy <- fisher1[, c(4,5)]
fisherSP <- SpatialPointsDataFrame(coords = xy, data = fisher1, proj4string = sp::CRS("+init=epsg:4326"))
mapview(fisherSP, zcol="individual.local.identifier", legend = TRUE, cex=5, lwd=2, map.type = c("OpenStreetMap.DE", "Esri.WorldShadedRelief"))
```
What is always amazing to me, living in the west, is how urban these Fisher's are. Whereas in the west, Fishers tend to be more of a wilderness specis. 

Next we will bringing Fisher data from a single individual, ID 1016, Fisher M1 (known as "RickyT"), into a MOVE object.
```{r}
dat <- read_csv("Martes pennanti LaPoint New York.csv") %>%
   filter(!is.na(`location-lat`)) %>%
   dplyr::select(x = `location-long`, y = `location-lat`,
           t = `timestamp`, id = `tag-local-identifier`) %>%
    filter(id %in% c(1465, 1466, 1072, 1078, 1016, 1469)) # for example 2
   dat_1 <- dat %>% filter(id == 1016)
```

_From Singer et al. (2019)..._
The function amt::make_track creates a track (the basic building block of the amt package), given the names of the columns containing x and y coordinates, time (t), and we can set a coordinate reference system (CRS). The original data were provided in geographical coordinates (EPSG code: 4326). Here, we shall transform this original CRS (using function amt::transform _ coords)
```{r}
dat_1 <- amt::make_track(dat_1, x, y, t, crs = sp::CRS("+init=epsg:4326")) %>%
     amt::transform_coords(sp::CRS("+init=epsg:5070"))

summarize_sampling_rate(dat_1)
```
_From Singer et al. (2019).._

We see that we have 8,957 total locations, the shortest interval between locations is 0.1 min and the largest time interval between locations is 1,208 min, with median interval length equal to roughly 2 min. Despite the 2 min temporal resolution, we choose to resample the track to 10 min with a tolerance of 1 min (amt::track _ resample), in order to conduct the analyses on the same temporal scale as the next example (some individuals had a median sampling rate of 10 min).

The function minutes from the package tidyverse and lubridate (Grolemund & Wickham, 2011), is used here to create an object of class Period that is then passed to amt::track_resample. Periods can be specified using all common time units; thus, it is straightforward to specify a sampling rate and an acceptable tolerance. We will also choose to keep only those bursts (subsets of the track with constant sampling rate, within the specified tolerance) with at least three relocations, the minimum required to calculate a turn angle amt::filter_min_n_burst). The following code implements those choices and translates from a point representation to a step (step length, turn angle) representation of the data. In the final line of the code snippet, we use the function amt::time_of_day (a wrapper around maptools:: sunriset and maptools::crepuscule; Bivand & Lewin‐Koh, 2017) to calculate if a location was taken during the day or night. If the argument include.crepuscule is set to TRUE, the function not only considers day and night, but also dawn and dusk.
```{r}
stps <- amt::track_resample(dat_1, rate = minutes(10), tolerance = minutes(1)) %>%
  filter_min_n_burst(min_n = 3) %>% steps_by_burst() %>%
  time_of_day(include.crepuscule = FALSE)

str(stps, width = 80, strict.width = "no", nchar.max = 80, give.attr = FALSE)
```

## Obtaining the NLCD data from FedData
Next, we will obtain the Landcover data from the National Landcover Database using the FedData pacakge. I encourage you to explore the FedData package here - they include several very handy spatial datasets including NLCD, Daymet, and national hydrography datasets. See here https://cran.r-project.org/web/packages/FedData/FedData.pdf 

The first set of commands are to create a blank raster that we can use as an extent to 'clip' the incoming NLCD data by. The key here is to remind yourself that the extent will have to be bigger than the actual extent of the Fisher data, because of the generation of availability samples during the SSF models.  Keep this in mind in your own research as well. For example, I had to go all the way back to the beginning and clip out a larger extent later.  
```{r}
#install.packages("FedData")
library(FedData)

## Create a Mask Raster based on the extent of Fisher. Note that I made it arbitrarily larger. 
extent(fisherSP)
fisherSP@proj4string

fisherSP2 <-spTransform(fisherSP, CRS("+init=epsg:5070"))
extent(fisherSP2)
mask.raster<-raster()
extent(mask.raster) <- c(xmin=1770000, xmax=1830000, ymin=2390000 , ymax=2430000)

res(mask.raster) = 30
#match projection to elc_habitat shapefile
projection(mask.raster)<- "+init=epsg:5070"
#set all values of mask.raster to zero
mask.raster[]<-0
plot(mask.raster)
plot(fisherSP2, add = TRUE)
```

Next, we use the get_nlcd command to obtain 1 tile of the NLCD dataset based on our input mask.raster. The full command can be specified with options such as the location for the raw.dir, the extraction.dir, raster.options, and whether you want to 'overwrite'(force redo). For example, here:
```
get_nlcd(mask.raster, label="landuse", year = 2011, dataset = "landcover", raw.dir = "/Users/mark.hebblewhite/Box Sync/Teaching/UofMcourses/WILD562/Spring2019/Labs/Lab9/2019/Fisher/raw/", extraction.dir = "/Users/mark.hebblewhite/Box Sync/Teaching/UofMcourses/WILD562/Spring2019/Labs/Lab9/2019/Fisher/extract/", raster.options = c("COMPRESS=DEFLATE", "ZLEVEL=9", "INTERLEAVE=BAND"), force.redo = F)
```
But, here, its easier to try the default settings
```{r}
get_nlcd(mask.raster, label="landuse", year = 2011, dataset = "landcover")
## note this may take sometime
land_use <- raster("EXTRACTIONS/landuse/NLCD/landuse_NLCD_2011_landcover.tif")
#extent(land_use) ## note the raster is the same as a the mask.raster above. 
```
Again, a reminder that the whole point here is to ensure you do NOT clip too closely to the extent of the spatial points dataframe - because of your availability sample. 

Lets see how we did:
```{r}
plot(land_use)
plot(fisherSP2, add=TRUE, type="p", color = "blue25", pch=12, cex = 0.5)
```
Like any landcover model, there are a number of categories, ~ 250 for the Conterminous United States (CONUS). But most of these do not occur in our New York study area, so many eo the land_use@data@attributes are blank. Note I have suppressed the code here to reduce space, but I explored the attributes using:
```
str(land_use)
land_use@data@attributes
levels(land_use)
```
The Landcover categories we do have are
```
ID OID Value      Count Red Green Blue    NLCD.2011.Land.Cover.Class
1     0   0     0 7854240512   0     0    0                  Unclassified
12   11  11    11  469012527   0     0    0                    Open Water
13   12  12    12    1599206   0     0    0            Perennial Snow/Ice
22   21  21    21  292251633   0     0    0         Developed, Open Space
23   22  22    22  131633826   0     0    0      Developed, Low Intensity
24   23  23    23   59456652   0     0    0   Developed, Medium Intensity
25   24  24    24   21426522   0     0    0     Developed, High Intensity
32   31  31    31  110507264   0     0    0                   Barren Land
42   41  41    41  973617734   0     0    0              Deciduous Forest
43   42  42    42 1037912310   0     0    0              Evergreen Forest
44   43  43    43  179845520   0     0    0                  Mixed Forest
53   52  52    52 1940362409   0     0    0                   Shrub/Scrub
72   71  71    71 1306961628   0     0    0                   Herbaceuous
82   81  81    81  597234572   0     0    0                   Hay/Pasture
83   82  82    82 1392218141   0     0    0              Cultivated Crops
91   90  90    90  347144473   0     0    0                Woody Wetlands
96   95  95    95  116679631   0     0    0 Emergent Herbaceuous Wetlands
```
_adapted from Singer et al._ 

We hypothesized that Fishers prefers forested wetlands over other landuse classes, based on previous work by Scott LaPoint. Armed with our NLCD landuse raster we create a layer called wet that is 1 for forested wetlands (category 90) and 0 otherwise (using the raster package; Hijmans, 2017).
```{r}
wet <- land_use == 90
names(wet) <- "wet"

##Lets zoom into Ricky T
rickyT.raster <- raster()
extent(rickyT.raster) <- c(xmin=1776500, xmax=1781500, ymin=2411000, ymax=2414000)

plot(wet, ext = rickyT.raster)
plot(fisherSP2, add=TRUE, type="p", color = "blue25", pch=12, cex = 0.5, ext = rickyT.raster)
```
Wow, Ricky surely seems to preferntially select 'wet' forests. 

## Exploratory Analyses of Step Lengths and Turning Angles

Before fitting our SSF, we will conduct some explortatory analyses of whether step lengtha and turning angles differ between wet and all other landcover models.  Our first step is to extract covariates for the start point of each step using the amt::extract_covariates() function. 

_from Singer et al_

Note that the function amt::extract _ covariates takes an argument where that indicates
whether covariate values should be extracted at the beginning or the end of a step (“both” can be used to extract the covariate at the start and the end of a step). Depending on the target process under investigation (habitat selection or movement), covariates might be extracted at the end of the step (habitat selection process) or at the start of the step
(movement process). If covariates are extracted at the end of the step, they are typically included in the model as main effects, to answer questions of the type: How do covariates influence where the animal moves? In contrary, if covariates are extracted at the beginning of the step, they are typically included in the model as an interaction with movement characteristics (step length, log of the step length, or the cosine of the turn angle), to test hypotheses of the type: Do animals move faster/more directed, if they start in a given habitat? Finally, covariate values at the start and the end of a step can also be included in the model as an interaction with each other, to test hypotheses of the type: Are animals more likely to stay in a given habitat, if they are already in that habitat?
```{r}
eda1 <- stps %>% extract_covariates(wet, where = "start") %>% mutate(landuse = factor(wet, levels = c(0, 1), labels = c("other", "forested wetland")))
```

Next, we make some summary plots of step length, turning angle as a function of day/night, and wet and other landcover types. We bundle them together using the cowplot package.  
```{r}
p1 <- eda1 %>% dplyr::select(landuse, tod = tod_end_, sl_, ta_) %>%
  gather(key, val, -landuse, -tod) %>%
  filter(key == "sl_") %>%
  ggplot(., aes(val, group = tod, fill = tod)) + geom_density(alpha = 0.5) +
  facet_wrap(~ landuse, nrow = 2) +
  xlab("Step length [m]") + theme_light() +
  ylab("Density") +
  theme(legend.title = element_blank())

p2 <- eda1 %>% dplyr::select(landuse, tod = tod_end_, sl_, ta_) %>%
  gather(key, val, -landuse, -tod) %>%
  filter(key == "ta_") %>%
  ggplot(., aes(val, group = tod, fill = tod)) + geom_density(alpha = 0.5) +
  facet_wrap(~ landuse, nrow = 2) +
  xlab("Turn angle") + theme_light() +
  theme(legend.title = element_blank(),
  axis.title.y = element_blank())

library(cowplot)
pg1 <- plot_grid(
  p1 + theme(legend.position = "none"),
  p2 + theme(legend.position = "none"), rel_widths = c(1, 1))
leg <- get_legend(p1)
plot_grid(pg1, leg, rel_widths = c(1, 0.1))
```
If you want to save the figure, use this
```
#ggsave("fig_eda_1_animal.pdf", width = 20, height = 18, units = "cm")
```
This figure represents Exploratory data analysis of one individual fisher, Ricky T (id: 1016): empirical distributions of step lengths (first column) and turning angles (second
column) are shown for forested wetland (second row) and other habitats (first row)
and for day and night (colors). 

We note that the underlying gamma distributions for the step lengths vary by time of day, and, by landcover types such that it seems the Fisher moves further at night and especially in forested wetlands. Similarly, we see differences in the turing angles where there is much stronger directional persistence during the night in forested wetlands.  We will see the coefficients for these differences in the fitting of the SSF model. 

## Fitting a Step Selection Function

_From Singer et al._

To fit SSFs, the observed covariates associated with observed steps are compared to covariates associated with random (or control) steps. Random steps can be generated by either:
(a) sampling from the observed turn step‐length and turn angle distribution (resulting in a traditional SSF al la Fortin et al. 2005), or 
(b) by fitting a parametric distribution to the observed step lengths (either a negative‐exponential, a halfnormal, a log‐normal, or a gamma; see Avgar et al., 2016, Appendix 2) and turn angles (a von Mises; Duchesne et al., 2015). 

As mentioned above, chosing b), an iSSF, is arguably less biased and also provides the user with a mechanistic movement model that can be used to simulate space use, and hence utilization distributions (Avgar et al., 2016; Signer et al., 2017). Currently, amt only implements the iSSFs with gamma and von Mises distributions. 

### amt::random_steps
Before we fit the ssf, we must first generate random steps using amt::random_steps(n=9) which choses 9 random points per Fisher location.  We then also extract covariates at the end points of each random step (the wet covariate here), and add time of day and the log of step length to each observation. We proceed by fitting a gamma distribution to the step
lengths and a von Mises distribution to the turn angles using maximum likelihood (Agostinelli & Lund, 2017; Delignette‐Muller & Dutang, 2015), and use these distributions to generate and pair nine random steps with each observed step. The number of random steps
effects the estimation error; the more the steps, the lower the error, but the higher the computational burden (Avgar et al., 2016). 
```{r}
m1 <-stps %>% amt::random_steps(n = 9) %>%
  amt::extract_covariates(wet) %>%
  amt::time_of_day(include.crepuscule = FALSE) %>%
  mutate(log_sl_ = log(sl_)) -> d1
```
To see what the random_steps() function did, take a look at the first 18 rows. 
```{r}
head(m1, n=18)
#str(m1)
```
The target variable case_ is one for observed steps and zero for random (or control) steps. Each step is paired with several (here 9) control steps that form together a stratum (indicated by strat(step_id_) in the model formula). The function amt::random_steps automatically creates a new column, step_id_ , that identifies different strata.

Note that it is challenging to visualize the SSF point generation process, and at this point, we note that the x1_ y1_ and x2_ and y2_ for the 'random' cases. I have tried to visualize the random steps here, but I think because of the short duration, they do not seem as dramatic.  
```{r}
m1$caseF <-as.factor(m1$case_)
ggplot(m1, aes(x2_ ,y2_, colour = caseF)) + geom_point(aes(size = caseF, colour = caseF))
xy2 <- m1[, c(6,7)]
fisherSSFPlot <- SpatialPointsDataFrame(coords = xy2, data = m1, proj4string = sp::CRS("+init=epsg:5070"))
mapview(fisherSSFPlot, zcol="caseF", legend = TRUE, cex="caseF", lwd=2, map.type = c("Esri.WorldShadedRelief", "OpenStreetMap.DE"))
``` 

### Fitting SSF models using amt::fit_issf()

Next we fit 3 different statistical Step dplyr::selection Function models using the amt::fit_issf to fit a conditional logistic regression model to the resulting data including
movement‐related covariates with the function amt::fit _ issf (a wrapper to survival::clogit; Therneau & Grambsch, 2000). 

We included three different combinations of different main effects, and their interactions, in 3 different SSF models below. These included the environmental covariate wet, the step length, and and the log of the step length (log_sl_ ) as modifiers of the shape parameter of the underlying gamma distribution. The estimated coefficient of sl_ and log_sl_ can be used to adjust the tentative shape estimate (i.e., the estimate of the shape parameter using the observed step lengths) of the underlaying gamma distribution for the step lengths. We also include interactions between wet and tod_ ,a factor with two levels—day (the reference category) and night, and between tod_ and log_sl_ .These interactions are included to the test the hypotheses that habitat dplyr::selection and displacement rate, respectively, differ between day and night.

We could have also included cosines of the turning angles and their interaction with day. This choice would modify the concentration parameter of the underlying von Mises distribution for the turning angles and allow the degree of directional persistence to depend on time of day; the data summarized in Figure 1 suggest that this could be a sensible
choice. For the sake of simplicity, however, we have assumed we have correctly modeled the degree of directional persistence and that it does not differ between day and night.

```{r}
m3 <- d1 %>% amt::fit_issf(case_ ~ wet + sl_ + wet:tod_end_+ sl_:tod_end_ + strata(step_id_))
m2 <- d1 %>% amt::fit_issf(case_ ~ wet + log_sl_ + wet:tod_end_+ log_sl_:tod_end_ + strata(step_id_))
m1 <- d1 %>% amt::fit_issf(case_ ~ wet + log_sl_ + sl_ + wet:tod_end_+ log_sl_:tod_end_ + sl_:tod_end_ + strata(step_id_))
```
Model 3 represents the habitat hypothesis that Fisher resource dplyr::selection differs for wet forests, and, differs between day and night (represented by the interaction between wet:tod_end_).  Movement hypotheses addressed as that step lenghts also differ between times of day, for example.  Here, the absolute step length, sl_, is modeled. 

Model 2 represents a similar hypothesis, but instead, with the log of step length. Finally, 
Model 1 includes both the untransformed step length and the log of step length. 

### Model Selection
We can conduct model dplyr::selection using AIC on these 3 models. First, I note however, that the Log-Likelihood for a conditional logistic regression model is not directly comparable to that of a traditional logistic regression model. So we cannot test, using AIC, whether conditioning on availability at every time step 'improves' model fit relative to a 'naive' logistic regression model. We will demonstrate this below when we explore mixed-effects SSF models. 
```{r}
AIC(m1$model, m2$model, m3$model)
```
So clearly, model1 is a bit better than model 2 and 3. Lets look at Model 1, and summarize/extract the coefficients.
```{r}
summary(m1)
s <- summary(m1$model)$coefficients
s
```
_from Singer et al. 2019_
Inspecting the fitted model, we make the following observations. (a) There is evidence to suggest that the animal prefers forested wetlands over other landuse classes, (b) there is no difference in Ricky's preference for wetlands between day and night, (c)
there is evidence to modify the shape of the gamma distribution fit to the observed step lengths (through the log of the step length), and (d) the modification of the shape parameter should be done separately for day and night, indicating that expected movement speeds differ between day and night.

We can also extract the coefficients and put them in a nice table:
```{r}
print(xtable::xtable(s, digits = 4,type = "latex", caption.placement = "top"))
```

### Calculating Movement Statistics

Besides inspecting the coefficients and their standard errors, we can calculate derived quantities, such as the expected speed. Because we included an interaction between parameters of the step‐length distribution and time of the day, we have to account for this interaction when calculating the expected speed for day and night. We begin by retrieving
the tentative parameter estimates (i.e., the estimated parameters before correcting for habitat dplyr::selection; see Avgar et al. (2016) for more details) for the gamma distribution of the step‐length distribution:
```{r}
shape <- sl_shape(m1)
scale <- sl_scale(m1)

shape_adj_day <- amt::adjust_shape(shape, coef(m1)["log_sl_"])
shape_adj_night <- amt::adjust_shape(shape, coef(m1)["log_sl_"]) +
coef(m1)["log_sl_:tod_end_night"]

scale_adj_day <- amt::adjust_scale(scale, coef(m1)["sl_"])
scale_adj_night <- amt::adjust_scale(scale, coef(m1)["sl_"]) + coef(m1)["sl_:tod_end_night"]

# speed
speed_day <- shape * scale_adj_day
speed_night <- shape * scale_adj_night

speed_day <- shape_adj_day * scale
speed_night <- shape_adj_night * scale

speed_day <- shape_adj_day * scale_adj_day
speed_night <- shape_adj_night * scale_adj_night

scale
shape

shape_adj_day
shape_adj_night
```


And we can compare the fits of the gamma distributions to day and night adjusted (for habitat dplyr::selection) coarsely using:
```{r}
x <- seq(1, 500, 1)
plot(x, dgamma(x, shape = shape_adj_night, scale = scale_adj_night), type = "l")
lines(x, dgamma(x, shape = shape_adj_day, scale = scale_adj_day), type = "l")
```
The expected speed for day and night is thus given by the product of the tentative scale parameter (no adjustment is needed here, because we did not include step length in the
model and hence assumed that the scale of the gamma distribution describing the step lengths remains unchanged) and the adjusted shape parameter. To obtain 95% confidence intervals for the mean speed, one can bootstrap the model m1 1,000 times by resampling (with replacement) the strata (for full code see Supporting information Data S1). Results suggest that Ricky T moves significantly faster during nights (11.0 m/min, 95% CI = 10.7, 11.4 m/min) than during
days (8.57 m/min, 95% CI = 7.8, 9.32 m/min).

For brevity, I do not demonstrate the bootstrapping procedure here, but refer you to Supporting Information from Singer et al. 2019. 


## Simulating the Utilization Distributions 

In a final step, we simulated space‐use from the fitted model m1 to obtain a model‐based estimate of the animal's utilization distribution (UD; Avgar et al., 2016; Signer et al., 2017). Generally, two types of UDs can be simulated: the transient UD and the steady‐state UD. The transient UD describes the expected space‐use distribution of the animal within a short time period and is hence conditional on the starting position. The steady‐state UD describes the expected space‐use distribution of the animal in the long‐term. In order to simulate UDs, one has to ensure that the animals stay within the study domain. 

We see three possible methods for achieving this goal–all implemented in amt: 
(a) use a covariate that attracts the animal toward one or more centers of activity (e.g., the squared distance to the mean of all coordinates), 
(b) use a very large landscape, or 
(c) use a wrapped landscape (torus). 

Here, we illustrate the simulation of steady‐state and transient UDs. For the steady‐state UD, we simulate from the first observed location 107 time steps on a toroid landscape,
once for day and once for night. For the transient UD, we are interested in the UD up to 10 hr after last observation, we therefore simulated 72 steps (at a 10 min sampling rate) 5 × 103 times. 

First, we crop out wet for a smaller area around just Ricky T using the amt::bbox() function which describes a bounding box. 
```{r}
wet_c <- crop(wet, amt::bbox(dat_1, spatial = TRUE, buff = 1e3))
```
First, we recall that since we have different model coefficients for day and night, we need to fit the movement and habitat kernels for day and night. 

Thus, we estimte the daytime movement kernel using the amt::movement_kernel(). This calculates a movement kernel from a fitted (i)SSF. The method is currently only implemented for the gamma distribution. Note, we use the tentative scale estimate and the shape estimate
adjusted for day.
```{r}
mk <- amt::movement_kernel(scale, shape_adj_day, wet_c)
plot(mk)
```

Second, we esitmate the habitat kernel which is calculated more or less like a traditional RSF model by multiplying resources with their corresponding coefficients from the fitted (i)SSF.That is for each pixel we calculate the estimated dplyr::selection coefficients times the resources and exponentiate the product. 
```{r}
hk <- amt::habitat_kernel(list(wet = coef(m1)["wet"]), wet_c)
plot(hk)
```

Next, we fit the simulated steady-state UD based on the movement model embedded within the SSF using amt:: simulate_ud(), which simulates a utilization distribution (UD) from a fitted Step-dplyr::selection Function. We time it because we are into that sort of thing.
```{r}
system.time(ssud_day <- amt::simulate_ud(
  mk, hk,
  as.numeric(stps[1, c("x1_", "y1_")]),
   n = 1e7))
 plot(ssud_day)
```

Finally, we simulate the transient UD which is based on some starting location and duration (72 steps here) using the amt:simulate_tud(). This is a conviencience wrapper arround simulate_ud to simulate transition UDs (i.e., starting at the same position many times and only simulate for a short time). In order to simulate the transient UD we have to repeatedly simulate short tracks starting at the same point, and then sum individual UDs and normalize, which we do with the function amt::simulate _ tud.
```{r}
system.time(tud_day <- amt::simulate_tud(mk, hk, as.numeric(stps[150, c("x1_", "y1_")]), n = 72, n_rep = 5e3))
plot(tud_day)
```

Now we repeat all the above steps for night
```{r}
 # night
 mk <- amt::movement_kernel(scale, shape_adj_night, wet_c)
 hk <- amt::habitat_kernel(list(wet = coef(m1)["wet"] + coef(m1)["wet:tod_end_night"]), wet_c)

 system.time(ssud_night <- amt::simulate_ud(
   mk, hk, as.numeric(stps[1, c("x1_", "y1_")]), n = 1e7))
plot(ssud_night)

system.time(tud_night <- amt::simulate_tud(mk, hk, as.numeric(stps[150, c("x1_", "y1_")]), n = 72, n_rep = 5e3))
plot(tud_day)
plot(tud1 <- crop(tud_day, extent(c(1778000, 1782000, 2412000, 2415000))))
plot(tud2 <- crop(tud_night, extent(c(1778000, 1782000, 2412000, 2415000))))
```

And finally, we bundle them all together in a nice figure
```{r}
pllog <- list(
  geom_raster(),
   coord_equal(),
  scale_fill_continuous(low = "white", high = "red", tran = "log10", na.value = "white"),
   scale_y_continuous(expand = c(0, 0)),
   scale_x_continuous(expand = c(0, 0)),
   theme_light(),
   theme(legend.position = "none"))

 pl <- list(
   geom_raster(),
   coord_equal(),
   scale_fill_continuous(low = "white", high = "red", na.value = "white"),
 scale_y_continuous(expand = c(0, 0)),
   scale_x_continuous(expand = c(0, 0)),
   theme_light(),
   theme(legend.position = "none"))

r1 <- data.frame(rasterToPoints(mk))
p1 <- ggplot(r1, aes(x, y, fill = d)) + pllog + ggtitle("Movement kernel (night)")

r2 <- data.frame(rasterToPoints(hk))
p2 <- ggplot(r2, aes(x, y, fill = layer)) + pl + ggtitle("Habitat kernel (night)")

r1 <- data.frame(rasterToPoints(tud1))
 p3 <- ggplot(r1, aes(x, y, fill = layer)) + pllog + ggtitle("Transient UD (day)")

 r2 <- data.frame(rasterToPoints(tud2))
p4 <- ggplot(r2, aes(x, y, fill = layer)) + pllog + ggtitle("Transient UD (night)")


r1 <- data.frame(rasterToPoints(ssud_day))
p5 <- ggplot(r1, aes(x, y, fill = layer)) + pl + ggtitle("Steady state UD (day)")

r2 <- data.frame(rasterToPoints(ssud_night))
p6 <- ggplot(r2, aes(x, y, fill = layer)) + pl + ggtitle("Steady state UD (night)")

cowplot::plot_grid(p1, p2, p3, p5, p4, p6, ncol = 2, labels = "AUTO")
```
This figure shows Simulated utilization distributions. To obtain simulated Utilization Distributions (UD), a movement kernel (panel a) and a habitat kernel (panel b) are needed. The movement kernel is always placed at the current position of the animal. The next step of the animal is then sampled with probability proportional to the product of two kernels. Expected differences in movement speeds between night and day are reflected in the transient UD (panels c and e) and to a lesser extend in steady‐state UD (panels d and f). Note, for better visualization, fills were log10 transformed for panels a, c, and e

Note that the transient UD's are slightly different from the paper because I started in a slightly different startign location. 

Finally, if we wanted to save them all:
```
ggsave("fig_one_animal1.pdf", height = 20, width = 24, units = "cm")
```

# Fitting SSF Models to Multiple Animals

Make sure we have all these packages loaded (not sure we need parallel), and we will conduct two-stage modeling of SSF's fit to individual Fishers in the same study area. 
``` 
library(ggplot2)
library(raster)
library(lubridate)
library(amt)
library(parallel)
```
We start again with the same data set (dat), containing data from six individual fishers. This time we are interested in quantifying among‐animal variability in the dplyr::selection coefficients. We proceed using nearly all the same steps as in the first example, but with a different data structure: data_ frames with list columns (Müller & Wickham, 2018). List columns are best thought of as regular columns of a data_ frame that are R lists and can contain any objects (in our case tracks and fitted models). The purrr::nest command can be used to nest data into a list column (Henry & Wickham, 2017).
```{r}
dat <- read_csv("Martes pennanti LaPoint New York.csv") %>%
 filter(!is.na(`location-lat`)) %>%
  dplyr::select(x = `location-long`, y = `location-lat`,
              t = `timestamp`, id = `tag-local-identifier`) %>%
   filter(id %in% c(1465, 1466, 1072, 1078, 1016, 1469))

 dat_all <- dat %>% nest(-id)
dat_all$sex <- c("f", "f", "f", "m", "m", "m")
dat_all
```
dat_all is now a nested data frame with 6 rows (one for each individual) and two columns. In the first column the animal id is given, and in the second column (by default named data) the relocations of the corresponding animal are saved.

We can now apply the steps as before for all animals. We first create a track for each animal and transform the coordinate reference system using the function amt::transform _ coords.
```{r}
 dat_all <- dat_all %>%
  mutate(trk = map(data, function(d) {
     amt::make_track(d, x, y, t, crs = sp::CRS("+init=epsg:4326")) %>%
       amt::transform_coords(sp::CRS("+init=epsg:5070"))}))
```
And summarize sampling rate
```{r}
dat_all %>% mutate(sr = lapply(trk, summarize_sampling_rate)) %>%
  dplyr::select(id, sr) %>% unnest
```
This time we see that some individuals have a 2 min sample rate and others a 10 min one. Thus, we decided to resample the tracks to the same sampling rate of 10 min (noting that (i)SSF inference is scale dependent; Signer et al., 2017) using amt::track _ resample.

Next, we are going to test for SSF as a function of 4 different landcover types now, more than just the wet forests. So we will create a reclassification matrix that lumps everything into 5 categories, with water/wet forests as the reference catetgory: water and wetland forests, developed open spaces, other developed areas, forests and shrubs, and crops. 
```{r}
  land_use <- raster("EXTRACTIONS/landuse/NLCD/landuse_NLCD_2011_landcover.tif")
  rcl <- cbind(c(11, 12, 21:24, 31, 41:43, 51:52, 71:74, 81:82, 90, 95),c(1, 1, 2, 3, 3, 3, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 1, 1))
# water, dev open, dev, barren, forest, shrub and herb, crops, wetlands
# 1: water, wetlands
# 2: developed (open)
# 3: developed (other)
# 5: forest, herbaceouse
 # 8: crops
 lu <- reclassify(land_use, rcl, right = NA)
 names(lu) <- "landuse"
plot(lu)
```

Next, we resample the tracks from 6 individual fishers to 10 minute steps, filter them into consecutive bursts of 3 or more 10 minute locations, create random steps (which by default goes back to the last setting used, which was n=9), extract covariate values from both the start and the end, and convert the landuse covariate to a factor all in one step.
```{r}
 m1 <- dat_all %>%
   mutate(steps = map(trk, function(x) {
     x %>% amt::track_resample(rate = minutes(10), tolerance = seconds(120)) %>%
      amt::filter_min_n_burst() %>%
       amt::steps_by_burst() %>% amt::random_steps() %>%
       amt::extract_covariates(lu, where = "both") %>%
       mutate(landuse_end = factor(landuse_end))
     }))
```

We then fit a simple SSF model to each individual. The main difference to the previous example here, is that the all the steps from above are wrapped into one mutate call. This call creates a new column to dat _ all called ssf. This is a list column and each entry in this column contains a fitted SSF. 
```{r}
m4 <- m1 %>% mutate(fit = map(steps, ~ amt::fit_issf(., case_ ~ landuse_end +
                                                           strata(step_id_))))
m4
```
m4 is still a data frame with one new column: ssf that is again a list column with a fitted SSF. 



```{r}
d2 <- m4 %>% mutate(coef = map(fit, ~ broom::tidy(.x$model))) %>%
   dplyr::select(id, sex, coef) %>% unnest %>%
  mutate(id = factor(id)) %>% group_by(term) %>%
  summarize(
     mean = mean(estimate),
    ymin = mean - 1.96 * sd(estimate),
     ymax = mean + 1.96 * sd(estimate))

d2$x <- 1:nrow(d2)
```
Finally, we make some fancy figures
```{r}
p1 <- m4 %>% mutate(coef = map(fit, ~ broom::tidy(.x$model))) %>%
   dplyr::select(id, sex, coef) %>% unnest %>%mutate(id = factor(id)) %>%
   ggplot(., aes(x = term, y = estimate, group = id, col = id, pch = sex)) +
   geom_rect(mapping = aes(xmin = x - .4, xmax = x + .4, ymin = ymin, ymax = ymax), data = d2, inherit.aes = FALSE,fill = "grey90") +geom_segment(mapping = aes(x = x - .4, xend = x + .4,y = mean, yend = mean), data = d2, inherit.aes = FALSE, size = 1) +
 geom_pointrange(aes(ymin = conf.low, ymax = conf.high),position = position_dodge(width = 0.7), size = 0.8) + geom_hline(yintercept = 0, lty = 2) +
   labs(x = "Habitat", y = "Relative selection Strength") + theme_light() +
  scale_x_discrete(labels = c("Dev(open)", "Dev(other)", "Natural", "Crops"))

p1
```
From here, it is easy to investigate coefficients for several animals and look at population‐level effects. The results suggest that there are some general population‐level trends. All fishers seem to prefer wetland forests and natural areas relative to developed areas (of either type), whereas considerable among‐animal variability in the coefficients for crops makes it difficult to draw firm conclusions about this landuse type. Lastly, there seems to be little differentiation based on sex.

And again, if we want to save we use:
```
ggsave("img/fig_all_animals.pdf", width = 24, height = 12, units = "cm")
```
# Mixed-effect cLogit Models

In the next set of exercises, I will build on the SSF case study of Singer et al., and conduct mixed-effects clogit analyses accounting for individual fishers as random effects similar to Lab 7.  Conceptually, adding random effects to conditional logistic regression models was challenging because there is no intercept.  Here are the first 2 papers that figured out how to add a random intercept for each individual animal (e.g.), however, it did so in MATLAB. So, its mostly inaccessible to biologists. 

  Craiu, R. V., T. Duchesne, D. Fortin, and S. Baillargeon. 2011. Conditional Logistic        Regression With Longitudinal Follow-up and Individual-Level Random Coefficients: A Stable   and Efficient Two-Step Estimation Method. Journal of Computational and Graphical            Statistics 20:767-784.

  Duchesne, T., D. Fortin, and N. Courbin. 2010. Mixed conditional logistic regression for    habitat dplyr::selection studies. Journal of Animal Ecology 79:548-555.

Since these initial papers, however, there have been a few big breakthrough’s lately with the mclogit package http://cran.r-project.org/web/packages/mclogit/mclogit.pdf  or the coxme package here http://cran.r-project.org/web/packages/coxme/coxme.pdf I just played around with both of these packages and they are actually. 

We will first need to 'unpack' the nested data frame from section 3 above into an expanded dataframe using the unnest command.  Then we will progress through a set of 3-4 different models and compare model interpretations to naive GLM models of the same kind. 
```{r}
fisher6 <- dat_all %>%
  mutate(steps = map(trk, function(x) {
    x %>% amt::track_resample(rate = minutes(10), tolerance = seconds(120)) %>%
      amt::filter_min_n_burst() %>%
      amt::steps_by_burst() %>% amt::random_steps() %>%
      amt::extract_covariates(lu, where = "both") %>%
      mutate(landuse_end = factor(landuse_end))
  })) %>%
  dplyr::select(id, steps) %>%
  unnest()

fisher6
head(fisher6)
```


Next, we will add a String variable for landuse_end recalling that, earlier, we defined the NLCD landcover according to:
  1: water, wetlands
  2: developed (open)
  3: developed (other)
  5: forest, herbaceouse
  8: crops
```{r}
head(fisher6$landuse_end)
fisher6$landuseName = ifelse(fisher6$landuse_end == 1, "Wet Forests", 
                             ifelse(fisher6$landuse_end == 2, "Developed Open", 
                             ifelse(fisher6$landuse_end == 3, "Developed Other", 
                             ifelse(fisher6$landuse_end == 5, "Natural", "Crops"))))
table(fisher6$landuseName, fisher6$landuse_end)
```

## Fit a naive GLM
First, we will fit a 'naive' GLM only focusing on the habitat processes, that is, habitat dplyr::selection for the landcover covaraites and compare them to the coefficients from the SSF fit to each individual Fisher and their two-stage population-level averages. 
```{r}
model1 <- glm(case_~ I(landuse_end), data=fisher6,family=binomial(link="logit"))
## I commented out these next few versions of the models fit to landuseName to make comparisons to the previously fit 6 fisher two-step models more comparable, though we have to then keep track of which landovers 2, 3, 5, and 8 are. 
#model1 <- glm(case_~ I(landuseName), data=fisher6,family=binomial(link="logit"))
#model1 <- glm(case_~ I(landuseName=="Developed Open") + I(landuseName=="Developed Other") +I(landuseName=="Natural")+I(landuseName=="Crops"), data=fisher6,family=binomial(link="logit"))
summary(model1)
```
This model gives us an intercept, which is interpreted as wet-forests, and we note that we also did not get an intercept in the SSF model above.  Lets now compare the coefficients to above:
```{r}
coef(model1)
naive_glm <- broom::tidy(model1) %>% 
  filter(!term=="(Intercept)") 

figNaive <- naive_glm %>%
  ggplot(., aes(x = term, y = estimate)) +
  geom_pointrange(aes(ymin = estimate - 1.96*std.error, ymax = estimate +1.96*std.error)) +
  labs(x = "Habitat", y = "Relative selection Strength") + 
    theme_light() +
  scale_x_discrete(labels = c("Dev(open)", "Dev(other)", "Natural", "Crops")) + geom_hline(yintercept = 0, lty = 2) + ylim(c(-3.75,1.5))
figNaive
```
First, we recall we are comparing the two-stage averaged SSF coefficients from the fisher SSF model above extracted in the object d2. This is simply the arithmetic mean of the 6 individual coefficients.  Second, note that the ORDER of landcover categories on the X axis are changed now, which is slightly annoying, and tough to remedy. But keep that in mind when looking at the next figure. 

```{r}
fig5 <- plot_grid(p1, figNaive)
fig5
```

We note that there are some similarities between coefficients, but, differences. 

  Developed Open is   -0.49 from the Naive Logit, and   -1.00 from the SSF
  Developed Other is  -1.82 from the Naive Logit, and   -2.20 from the SSF
  Natural is          +0.033 from the Naive Logit, and  +0.044 from the SSF
  CropsDeveloped Open -0.25 from the Naive Logit, and   -0.69 from the SSF

The difference in the interpretation from the different parameters highlights the 'effect' of movement, so to speak, on dplyr::selection for covariates. Again, here, note we are not considering any differences between male or female, or night or day.  For example, one could conclude that the biggest difference is in the dplyr::selection of Developed Open, which we woudl underestimate the avoidance of if we failed to consider the movement processes. 

## Fitting a 'Naive' cLogit Model 

Next, we will fit a 'naive' clogit model, that is, a model that does not account for any differences between individuals and treats all step_id_'s as independent. Basically ignoring any random effects structure of individual fishers in this case. 
_from https://rdrr.io/cran/survival/man/clogit.html_ 
It turns out that the loglikelihood for a conditional logistic regression model = loglik from a Cox model with a particular data structure. Proving this is a nice homework exercise for a PhD statistics class; not too hard, but the fact that it is true is surprising.

When a well tested Cox model routine is available many packages use this ‘trick’ rather than writing a new software routine from scratch, and this is what the clogit routine does. In detail, a stratified Cox model with each case/control group assigned to its own stratum, _time set to a constant_, status of 1=case 0=control, and using the exact partial likelihood has the same likelihood formula as a conditional logistic regression. The clogit routine creates the necessary dummy variable of times (all 1) and the strata, then calls coxph.

The computation of the exact partial likelihood can be very slow, however. If a particular strata had say 10 events out of 20 subjects we have to add up a denominator that involves all possible ways of choosing 10 out of 20, which is 20!/(10! 10!) = 184756 terms. Gail et al describe a fast recursion method which partly ameliorates this; it was incorporated into version 2.36-11 of the survival package. The computation remains infeasible for very large groups of ties, say 100 ties out of 500 subjects, and may even lead to integer overflow for the subscripts – in this latter case the routine will refuse to undertake the task. The Efron approximation is normally a sufficiently accurate substitute.

First we have to create a unique stratum ID for each set of steps for each individual animal. Right now, there is a case_ field, step_id_ field, and id_ field - but, the step_id_ field repeats for each animal ID. We do this by creating a new stratum field called stratum by pasting together fisher ID and step ID. 
```{r}
require(survival)
head(fisher6)
## Look at the number of step_id_'s for each id
fisher6 %>% group_by(step_id_) %>% summarize(n=n())
fisher6$stratum <- paste(fisher6$id, fisher6$step_id_)
```
So indeed, we see that there are 66 rows of data for each step ID because there are 6 individuals. 

```{r}
clogit1 <- clogit(case_ ~ I(landuse_end) + strata(stratum), data = fisher6)
#clogit1 <- clogit(case_ ~ I(landuseName=="Developed Open") + I(landuseName=="Developed Other") +I(landuseName=="Natural")+I(landuseName=="Crops") + strata(stratum), data = fisher6)
summary(clogit1)
coef(clogit1)
# tidy up coefficients
clogit_1 <- broom::tidy(clogit1) %>% 
  filter(!term=="(Intercept)") 
## make a figure
figclogit1 <- clogit_1 %>%
  ggplot(., aes(x = term, y = estimate)) +
  geom_pointrange(aes(ymin = estimate+ 1.96*std.error, ymax = estimate-1.96*std.error)) +
  labs(x = "Habitat", y = "Relative selection Strength") + 
    theme_light() +
  scale_x_discrete(labels = c("Dev(open)", "Dev(other)", "Natural", "Crops")) + geom_hline(yintercept = 0, lty = 2) + ylim(c(-3.75,1.5))
figclogit1
plot_grid(p1, figclogit1)
```

The coefficients are a bit different, but still, quite close even with the cluster for each individual Stratum. 

## Mixed-effect cLogit Models

We can currently fit mixed-effects clogit models using two R packages.

coxme and
mclogit. 

First, we will use the coxme package to fit a mixed-effects conditional logistic regression model, accounting for the random effect of individual Fisher ID in this case. First, we ensure we have the coxme package loaded. See here for more information: https://cran.r-project.org/web/packages/coxme/vignettes/coxme.pdf 
```{r}
require(coxme)
```
There is no 'convenient' wrapper around the coxme(Surv()) function like in clogit above.  Thus, first, we need to make a 'fake' time variable to trick the Cox-proportional hazards model that time is irrelevant in your conditional logistic model. We do this by adding a new variable, time_ to the fisher6 data frame above. This is actually what the clogit wrapper around survival is doing, we just don't know it. 
```{r}
fisher6$time_ <- ifelse(fisher6$case_ == 0, 2, 1)   #2 for control, 1 for case
table(fisher6$time_, fisher6$case_)

clogitM1<- coxme(Surv(time_,case_) ~ I(landuse_end) + strata(stratum) + (1|id), data=fisher6)
AIC(clogitM1)
summary(clogitM1)
```

We note, importantly, that the random effects variance is quite low, nearly zero. This means there is very little variation between individual Fishers in selection.  This becomes important later. 

Second, I tried to fit a similar model using the mclogit package. However, the current implementation of random effects is limited to the PQL technique, which requires large cluster sizes. Thus, here, we do not have large enough clusters with only 9 random points. We see an error message accordingly. 

```{r, eval = FALSE, echo =FALSE}
#The first column contains the choice counts or choice indicators (alternative is chosen=1, is not chosen=0). The second column contains unique numbers for each choice set.
# create a used / avail 1, 0 variable
fisher6$used_ <- ifelse(fisher6$case_ == 0, 0, 1) 
table(fisher6$time_, fisher6$used_)

# convert stratum to a number
fisher6$stratumN <- as.factor(fisher6$stratum)
levels(fisher6$stratumN) <- 1:length(levels(fisher6$stratumF))
fisher6$stratumN <- as.numeric(fisher6$stratumN)

# first fit a mclogit Test model wtih no random effect. 
mclogitTest <- mclogit(cbind(used_, stratumN) ~I(landuse_end), data=fisher6)

mclogitTest2 <- mclogit(cbind(used_, stratumN) ~I(landuse_end), random=~ 1|id, data=fisher6)
summary(mclogitTest2)
str(mclogitTest2)
```

Plotting the coefficients from coxme (or mclogit) are more difficult because there is no tidy approach for objects generated by coxme models, but we can quickly compare the coefficients directly here, where column 1, 2, and 3 are the naive GLM, naive clogit, and mixed-effect cLogit models. And compare them to the coefficients for the two-step models from Singer et al. 
```{r}
v1<-model1$coefficients[2:5]
v2<-coef(clogit1)
v3<-coef(clogitM1)
v4<-d2$mean
coefSum <- as.data.frame(cbind(v1, v2, v3, v4))
names(coefSum) <- c("Naive", "clogit", "coxme", "two-stage iSSF")
coefSum
```
Recall that landuse_end 2, 3, 5 and 8 correspond to developed (open), developed (other), natural, and cropland. And that the forested wet areas are the 'reference' category. 

We note that there are some important differences here between the 'naive' , clogit/coxme/mcclogit, and  two-stage iSSF model. The reason why there are few differences between clogit, the coxme or mcclogit is because of the rather low variance in the summary of clogitM1 for individual Fisher's we saw above. 

However, there are - just like Lab 7 - differences between the two-staged model, d2, and the coxme/clogit models for landcover types 3 and 5, in particular.  

## Model Selection
```{r}
AIC(model1, clogit1, clogitM1)
```
This clearly confirms that really, the mixed-effect model structure is not necessary and we are fine making inferences using the 'naive' cLogit Model. Finally, we should compare manually the sum of the individual model AIC values from the two-stage modeling to really undestand if this clogit model is better than the two-step. However, these results may be idiosyncratically dependent on the small sample size of Fisher's here, 6, and the limited variation in response to just one categorical covariate.  Often, addition of mixed-effects models improves model fit substantially. 

# Homework

Conduct an SSF model for JUST wolves in the Cascade, Red Deer and Bow Valley wolf packs for some covariates that we have used this semester.  Pick one season as well, and test whether there are differences in movement during day and night. 

```{r}
wolfGPS <- read.csv("wolfGPS.csv")
head(wolfGPS)
ggplot(wolfGPS, aes(X_COORD1, Y_COORD1, colour = WOLFNAME)) +geom_point()
ggplot(wolfGPS, aes(X_COORD1, Y_COORD1, colour = PACK)) +geom_point()
```



